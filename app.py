"""
Speech-to-Textã¨LLMã‚’é€£æºã•ã›ã‚‹Streamlitã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
"""
import os
import time
import threading
import queue
import logging
from typing import List, Tuple

import streamlit as st
from dotenv import load_dotenv

from speech_to_text import SpeechToTextStreaming
from llm_manager import LLMManager

# ãƒ­ã‚®ãƒ³ã‚°ã®è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("app.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰é–“ã§å…±æœ‰ï¼‰
_is_listening = False
_transcript_queue = queue.Queue()
_llm_manager = None  # LLMãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ä¿æŒ
_transcripts = []  # æ–‡å­—èµ·ã“ã—ã‚’ä¿å­˜ã™ã‚‹ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
_responses = []  # å¿œç­”ã‚’ä¿å­˜ã™ã‚‹ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
_current_transcript = ""  # ç¾åœ¨ã®æ–‡å­—èµ·ã“ã—ã‚’ä¿å­˜ã™ã‚‹ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
_current_response = ""  # ç¾åœ¨ç”Ÿæˆä¸­ã®LLMå¿œç­”ã‚’ä¿å­˜ã™ã‚‹ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
_update_ui = False  # UIã®æ›´æ–°ãƒ•ãƒ©ã‚°
_last_ui_update_time = time.time()  # æœ€å¾Œã«UIã‚’æ›´æ–°ã—ãŸæ™‚é–“
_force_update = False  # å¼·åˆ¶æ›´æ–°ãƒ•ãƒ©ã‚°
_is_generating = False  # LLMå¿œç­”ç”Ÿæˆä¸­ãƒ•ãƒ©ã‚°
_accumulated_context = ""  # Global variable to accumulate short-turn transcripts

# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ã®çŠ¶æ…‹ä¿å­˜
_STATE_FILE = "app_state.json"

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if "stt" not in st.session_state:
    st.session_state.stt = None

if "llm" not in st.session_state:
    st.session_state.llm = None

if "is_listening" not in st.session_state:
    st.session_state.is_listening = False

if "transcripts" not in st.session_state:
    st.session_state.transcripts = []

if "responses" not in st.session_state:
    st.session_state.responses = []

if "current_transcript" not in st.session_state:
    st.session_state.current_transcript = ""

if "transcript_queue" not in st.session_state:
    st.session_state.transcript_queue = queue.Queue()

if "response_thread" not in st.session_state:
    st.session_state.response_thread = None

if "last_update_time" not in st.session_state:
    st.session_state.last_update_time = time.time()

if "current_response" not in st.session_state:
    st.session_state.current_response = ""

if "is_generating" not in st.session_state:
    st.session_state.is_generating = False

# LLMã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
AIZUCHI_SYSTEM_PROMPT = """
ã‚ãªãŸã¯ä¼šè©±ã®ç›¸æ‰‹ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€ã«å¯¾ã—ã¦ã€ç›¸æ§Œã‚’æ‰“ã¤ã‚ˆã†ã«çŸ­ãè¿”ç­”ã—ã¦ãã ã•ã„ã€‚
ä¾‹ãˆã°ã€Œãªã‚‹ã»ã©ã€ã€Œãã†ã§ã™ã­ã€ã€Œç¢ºã‹ã«ã€ã€Œãã‚Œã¯èˆˆå‘³æ·±ã„ã§ã™ã­ã€ãªã©ã®çŸ­ã„ç›¸æ§Œã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
è¿”ç­”ã¯å¿…ãš1ã€œ3èªç¨‹åº¦ã®çŸ­ã„ç›¸æ§Œã«ã—ã¦ãã ã•ã„ã€‚é•·ã„èª¬æ˜ã‚„è³ªå•ã¯é¿ã‘ã¦ãã ã•ã„ã€‚
"""

CONVERSATION_SYSTEM_PROMPT = """
ã‚ãªãŸã¯ä¼šè©±ã®ç›¸æ‰‹ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€ã«å¯¾ã—ã¦ã€è‡ªç„¶ãªä¼šè©±ã‚’ç¶šã‘ã‚‹ã‚ˆã†ã«è¿”ç­”ã—ã¦ãã ã•ã„ã€‚
è³ªå•ã«ã¯ç­”ãˆã€æ„è¦‹ã«ã¯å…±æ„Ÿã‚„åˆ¥ã®è¦–ç‚¹ã‚’æä¾›ã—ã€ä¼šè©±ã‚’ç™ºå±•ã•ã›ã¦ãã ã•ã„ã€‚
è¿”ç­”ã¯ç°¡æ½”ã§è‡ªç„¶ãªä¼šè©±èª¿ã«ã—ã¦ãã ã•ã„ã€‚
"""

# ä¼šè©±ã‚¿ãƒ¼ãƒ³åˆ¤å®šç”¨ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
TURN_DETECTION_PROMPT = """
ã‚ãªãŸã¯ä¼šè©±åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€ã‚’åˆ†æã—ã€ãã‚ŒãŒå®Œçµã—ãŸç™ºè¨€ã‹ã€ç¶šããŒã‚ã‚‹é€”ä¸­ã®ç™ºè¨€ã‹ã‚’åˆ¤æ–­ã—ã¦ãã ã•ã„ã€‚

ä»¥ä¸‹ã®ç´”ç²‹ãªJSONã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚ä»–ã®èª¬æ˜ã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ï¼š
{
  "continueConversation": true/false,
  "acknowledgement": "é©åˆ‡ãªçŸ­ã„ç›¸æ§Œã‚„è¿”äº‹"
}

åˆ¤æ–­åŸºæº–ï¼š
- "continueConversation": false â†’ ç™ºè¨€ãŒå®Œçµã—ã¦ã„ã‚‹ï¼ˆè³ªå•ã‚„æ„è¦‹ãŒæ˜ç¢ºã«è¿°ã¹ã‚‰ã‚Œã¦ã„ã‚‹ï¼‰
- "continueConversation": true â†’ ç™ºè¨€ãŒé€”ä¸­ã¾ãŸã¯ç¶šããŒã‚ã‚‹ï¼ˆè¨€ã„ã‹ã‘ã¦æ­¢ã¾ã£ã¦ã„ã‚‹ã€å˜èªã ã‘ã®ç™ºè¨€ãªã©ï¼‰

ä¾‹ï¼š
- ã€Œä»Šæ—¥ã¯ã©ã‚“ãªå¤©æ°—ã§ã™ã‹ï¼Ÿã€â†’ {"continueConversation": false, "acknowledgement": "ä»Šæ—¥ã®å¤©æ°—ã«ã¤ã„ã¦ãŠç­”ãˆã—ã¾ã™"}
- ã€Œä»Šæ—¥ã¯...ã€â†’ {"continueConversation": true, "acknowledgement": "ã¯ã„"}
- ã€Œãã‚Œã£ã¦ã€â†’ {"continueConversation": true, "acknowledgement": "ã¯ã„ï¼Ÿ"}

ä¼šè©±ãŒå®Œçµã—ã¦ã„ã‚‹å ´åˆã¯å¿…ãšfalseã‚’è¿”ã—ã¦ãã ã•ã„ã€‚ç‰¹ã«è³ªå•ã‚„æ˜ç¢ºãªæ„è¦‹è¡¨æ˜ãŒã‚ã£ãŸå ´åˆã¯å¿…ãšfalseã§ã™ã€‚
"""

def initialize_stt():
    """
    Speech-to-Textã®åˆæœŸåŒ–
    """
    project_id = os.getenv("GOOGLE_CLOUD_PROJECT")
    if not project_id:
        error_msg = "ç’°å¢ƒå¤‰æ•°GOOGLE_CLOUD_PROJECTãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
        logger.error(error_msg)
        st.error(error_msg)
        return None
    
    try:
        logger.info(f"Speech-to-Textã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID: {project_id}")
        stt = SpeechToTextStreaming(
            project_id=project_id,
            language_code="ja-JP",
            use_short_model=False
        )
        logger.info("Speech-to-Textã®åˆæœŸåŒ–ã«æˆåŠŸã—ã¾ã—ãŸã€‚")
        return stt
    except Exception as e:
        error_msg = f"Speech-to-Textã®åˆæœŸåŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
        logger.error(error_msg)
        st.error(error_msg)
        return None

def initialize_llm():
    """
    LLMã®åˆæœŸåŒ–
    """
    global _llm_manager
    try:
        logger.info("LLMã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚")
        llm = LLMManager()
        _llm_manager = llm  # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã«ä¿å­˜
        logger.info("LLMã®åˆæœŸåŒ–ã«æˆåŠŸã—ã¾ã—ãŸã€‚")
        return llm
    except Exception as e:
        error_msg = f"LLMã®åˆæœŸåŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
        logger.error(error_msg)
        st.error(error_msg)
        return None

def on_speech_result(transcript: str, is_final: bool):
    """
    éŸ³å£°èªè­˜çµæœã‚’å—ã‘å–ã‚‹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°
    
    Args:
        transcript: èªè­˜ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        is_final: æœ€çµ‚çµæœã‹ã©ã†ã‹
    """
    global _transcript_queue, _current_transcript, _update_ui, _last_ui_update_time
    
    # ç¾åœ¨ã®æ–‡å­—èµ·ã“ã—ã‚’æ›´æ–°
    _current_transcript = transcript
    
    # ãƒ­ã‚°å‡ºåŠ›
    if is_final:
        logger.info(f"éŸ³å£°èªè­˜çµæœï¼ˆæœ€çµ‚ï¼‰: {transcript}")
    else:
        logger.debug(f"éŸ³å£°èªè­˜çµæœï¼ˆä¸­é–“ï¼‰: {transcript}")
    
    # æœ€çµ‚çµæœã®å ´åˆã¯ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
    if is_final and transcript.strip():
        _transcript_queue.put(transcript)
        _update_ui = True  # UIã®æ›´æ–°ãƒ•ãƒ©ã‚°ã‚’ã‚»ãƒƒãƒˆ
        _last_ui_update_time = time.time()  # æœ€å¾Œã®æ›´æ–°æ™‚é–“ã‚’è¨˜éŒ²

def on_llm_stream(chunk: str):
    """
    LLMã‹ã‚‰ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã‚’å‡¦ç†ã™ã‚‹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°
    
    Args:
        chunk: LLMã‹ã‚‰ã®ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯
    """
    global _current_response, _update_ui, _last_ui_update_time, _force_update, _is_generating
    
    if chunk:
        _current_response += chunk
        _update_ui = True
        _last_ui_update_time = time.time()
        
        try:
            # ã“ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã‹ã‚‰ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’æ›´æ–°
            st.session_state.current_response = _current_response
            # å¼·åˆ¶æ›´æ–°è¦æ±‚
            _force_update = True
        except Exception as e:
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–ï¼ˆåˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã‹ã‚‰ã®ã‚¢ã‚¯ã‚»ã‚¹ã§ç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§ã‚ã‚Šï¼‰
            logger.warning(f"ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®æ›´æ–°ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
            pass
    
    logger.debug(f"LLMã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°: {chunk}")  # è©³ç´°ãªãƒ­ã‚°

def save_state_to_file():
    """
    ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã®çŠ¶æ…‹ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹
    """
    global _transcripts, _responses, _current_transcript, _current_response
    
    state = {
        "transcripts": _transcripts,
        "responses": _responses,
        "current_transcript": _current_transcript,
        "current_response": _current_response,
        "timestamp": time.time()
    }
    
    try:
        with open(_STATE_FILE, "w", encoding="utf-8") as f:
            import json
            json.dump(state, f, ensure_ascii=False, indent=2)
        logger.info(f"çŠ¶æ…‹ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸã€‚è»¢è¨˜æ•°: {len(_transcripts)}, å¿œç­”æ•°: {len(_responses)}")
    except Exception as e:
        logger.error(f"çŠ¶æ…‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

def load_state_from_file():
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã®çŠ¶æ…‹ã‚’èª­ã¿è¾¼ã‚€
    """
    global _transcripts, _responses, _current_transcript, _current_response
    
    if not os.path.exists(_STATE_FILE):
        logger.info("çŠ¶æ…‹ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        return False
    
    try:
        with open(_STATE_FILE, "r", encoding="utf-8") as f:
            import json
            state = json.load(f)
            
        _transcripts = state.get("transcripts", [])
        _responses = state.get("responses", [])
        _current_transcript = state.get("current_transcript", "")
        _current_response = state.get("current_response", "")
        
        logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰çŠ¶æ…‹ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚è»¢è¨˜æ•°: {len(_transcripts)}, å¿œç­”æ•°: {len(_responses)}")
        return True
    except Exception as e:
        logger.error(f"çŠ¶æ…‹ã®ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return False

def process_transcripts():
    """
    éŸ³å£°èªè­˜çµæœã‚’å‡¦ç†ã™ã‚‹ã‚¹ãƒ¬ãƒƒãƒ‰é–¢æ•°
    """
    global _is_listening, _transcript_queue, _llm_manager, _transcripts, _responses, _update_ui, _last_ui_update_time, _force_update, _current_response, _is_generating, _accumulated_context
    
    logger.info("æ–‡å­—èµ·ã“ã—å‡¦ç†ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
    import json
    import re
    
    while _is_listening:
        try:
            if not _transcript_queue.empty():
                transcript = _transcript_queue.get(timeout=0.1)
                logger.info(f"ã‚­ãƒ¥ãƒ¼ã‹ã‚‰å–å¾—ã—ãŸæ–‡å­—èµ·ã“ã—: {transcript}")
                
                # ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚„è³ªå•å½¢å¼ã«åŸºã¥ãç°¡æ˜“åˆ¤å®šï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¨ã—ã¦ï¼‰
                has_question_mark = "?" in transcript or "ï¼Ÿ" in transcript
                is_short = len(transcript.strip()) < 10
                likely_question = any(q in transcript for q in ["ä½•", "ã©ã†", "ãªãœ", "ã„ã¤", "ã©ã“", "ã ã‚Œ", "èª°", "ã§ã™ã‹"])
                
                # Call LLM to decide turn-taking
                turn_response = _llm_manager.call_model(
                    prompt=transcript,
                    system_prompt=TURN_DETECTION_PROMPT,
                    model="gemini-2.0-flash-lite",  # ã‚ˆã‚Šè»½é‡ãªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
                    stream=False
                )
                logger.info(f"ã‚¿ãƒ¼ãƒ³åˆ¤å®šçµæœ: {turn_response}")
                
                # JSONãƒ‘ãƒ¼ã‚¹å‡¦ç†ã®æ”¹å–„
                continue_conversation = True  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                ack = "ãªã‚‹ã»ã©"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                
                try:
                    # æ­£è¦è¡¨ç¾ã§JSONã‚’æŠ½å‡º
                    json_match = re.search(r'\{.*?\}', turn_response.replace('\n', ' '), re.DOTALL)
                    if json_match:
                        try:
                            turn_data = json.loads(json_match.group(0))
                            continue_conversation = turn_data.get("continueConversation", True)
                            ack = turn_data.get("acknowledgement", "ãªã‚‹ã»ã©")
                        except json.JSONDecodeError:
                            # JSONè§£æã«å¤±æ•—ã—ãŸå ´åˆã¯ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨
                            continue_conversation = not (has_question_mark or likely_question)
                            
                    else:
                        # JSONãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨
                        continue_conversation = not (has_question_mark or likely_question)
                        if "acknowledgement" in turn_response:
                            ack_match = re.search(r'"acknowledgement":\s*"([^"]+)"', turn_response)
                            if ack_match:
                                ack = ack_match.group(1)
                except Exception as e:
                    # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨
                    logger.error(f"ã‚¿ãƒ¼ãƒ³åˆ¤å®šã®è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    continue_conversation = not (has_question_mark or likely_question)
                
                # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ­ã‚¸ãƒƒã‚¯ã«ã‚ˆã‚‹è¿½åŠ ãƒã‚§ãƒƒã‚¯
                if likely_question or has_question_mark:
                    # è³ªå•ã®å½¢å¼ãŒæ˜ç¢ºãªå ´åˆã¯ã€å¸¸ã«falseã«å¼·åˆ¶
                    continue_conversation = False
                    logger.info(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ­ã‚¸ãƒƒã‚¯ã«ã‚ˆã‚Šä¼šè©±ç¶™ç¶šã‚’Falseã«è¨­å®šï¼ˆè³ªå•æ¤œå‡ºï¼‰")
                
                # çµæœã‚’ãƒ­ã‚°ã«è¨˜éŒ²
                logger.info(f"æœ€çµ‚åˆ¤å®š: ä¼šè©±ç¶™ç¶š={continue_conversation}, ç›¸æ§Œ=\"{ack}\"")
                
                if continue_conversation:
                    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒç¶™ç¶šä¸­: çŸ­ã„ç›¸æ§Œã‚’è¿”ã—ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è“„ç©
                    _accumulated_context += " " + transcript
                    _responses.append(ack)
                    _transcripts.append(transcript)
                    
                    # ç›¸æ§Œã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
                    _current_response = ""  # ç¾åœ¨ã®å¿œç­”ã‚’ãƒªã‚»ãƒƒãƒˆ
                    for char in ack:
                        _current_response += char  # æ–‡å­—ã‚’è¿½åŠ 
                        on_llm_stream(char)
                        time.sleep(0.01)  # æ–‡å­—ã”ã¨ã«è‹¥å¹²ã®é…å»¶
                    
                    logger.info(f"ä¼šè©±ç¶™ç¶šä¸­: è“„ç©å†…å®¹=\"{_accumulated_context}\"")
                else:
                    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€ãŒå®Œäº†: å®Œå…¨ãªå¿œç­”ã‚’ç”Ÿæˆ
                    combined_prompt = _accumulated_context + " " + transcript if _accumulated_context.strip() else transcript
                    logger.info(f"ä¼šè©±å®Œäº†: å®Œå…¨ãªå¿œç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚å…¥åŠ›=\"{combined_prompt}\"")
                    
                    # LLMå¿œç­”ç”Ÿæˆã®é€²è¡ŒçŠ¶æ³ã‚’ç¤ºã™ãƒ•ãƒ©ã‚°ã‚’è¨­å®š
                    _is_generating = True
                    _current_response = ""  # ç¾åœ¨ã®å¿œç­”ã‚’ãƒªã‚»ãƒƒãƒˆ
                    
                    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚’ä½¿ç”¨ã—ã¦LLMã‚’å‘¼ã³å‡ºã—
                    _llm_manager.call_model(
                        prompt=combined_prompt,
                        system_prompt=CONVERSATION_SYSTEM_PROMPT,
                        model="gemini-2.0-flash-lite",
                        stream=True,
                        stream_callback=on_llm_stream
                    )
                    
                    # æ³¨ï¼šã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã®å ´åˆã€å¿œç­”ã¯on_llm_streamã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’é€šã˜ã¦å‡¦ç†ã•ã‚Œã€
                    # ãã®çµæœã‚’_current_responseã«è“„ç©ã—ã€æœ€çµ‚çš„ã«_responsesã«è¿½åŠ ã™ã‚‹
                    _responses.append(_current_response)
                    _transcripts.append(combined_prompt)
                    _accumulated_context = ""  # è“„ç©ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªã‚¢
                
                # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ•ãƒ©ã‚°ã‚’æ›´æ–°
                _is_generating = False
                _update_ui = True
                _force_update = True
                _last_ui_update_time = time.time()
                save_state_to_file()
            
            time.sleep(0.1)  # ãƒãƒ¼ãƒªãƒ³ã‚°é–“éš”
            
        except queue.Empty:
            time.sleep(0.1)
        except Exception as e:
            logger.error(f"æ–‡å­—èµ·ã“ã—ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            logger.exception(e)  # ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’è¨˜éŒ²
            time.sleep(1)  # ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã¯å°‘ã—å¾…æ©Ÿ
    
    logger.info("æ–‡å­—èµ·ã“ã—å‡¦ç†ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’çµ‚äº†ã—ã¾ã™ã€‚")

def start_listening():
    """
    éŸ³å£°èªè­˜ã‚’é–‹å§‹ã™ã‚‹
    """
    global _is_listening
    
    if st.session_state.is_listening:
        logger.info("ã™ã§ã«éŸ³å£°èªè­˜ã‚’é–‹å§‹ã—ã¦ã„ã¾ã™ã€‚")
        return
    
    logger.info("éŸ³å£°èªè­˜ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
    
    # Speech-to-Textã®åˆæœŸåŒ–
    if not st.session_state.stt:
        logger.info("Speech-to-Textã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚")
        st.session_state.stt = initialize_stt()
    
    # LLMã®åˆæœŸåŒ–
    if not st.session_state.llm:
        logger.info("LLMã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚")
        st.session_state.llm = initialize_llm()
    
    if not st.session_state.stt or not st.session_state.llm:
        error_msg = "åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
        logger.error(error_msg)
        st.error(error_msg)
        return
    
    # éŸ³å£°èªè­˜ã‚’é–‹å§‹
    logger.info("ãƒã‚¤ã‚¯ã‹ã‚‰ã®éŸ³å£°èªè­˜ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
    st.session_state.stt.start_listening(callback=on_speech_result)
    st.session_state.is_listening = True
    _is_listening = True
    
    # æ–‡å­—èµ·ã“ã—å‡¦ç†ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹
    logger.info("æ–‡å­—èµ·ã“ã—å‡¦ç†ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
    st.session_state.response_thread = threading.Thread(target=process_transcripts)
    st.session_state.response_thread.daemon = True
    st.session_state.response_thread.start()

def stop_listening():
    """
    éŸ³å£°èªè­˜ã‚’åœæ­¢ã™ã‚‹
    """
    global _is_listening
    
    if not st.session_state.is_listening:
        logger.info("éŸ³å£°èªè­˜ã¯ã™ã§ã«åœæ­¢ã—ã¦ã„ã¾ã™ã€‚")
        return
    
    logger.info("éŸ³å£°èªè­˜ã‚’åœæ­¢ã—ã¾ã™ã€‚")
    
    # éŸ³å£°èªè­˜ã‚’åœæ­¢
    if st.session_state.stt:
        st.session_state.stt.stop_listening()
    
    st.session_state.is_listening = False
    _is_listening = False
    
    # ã‚¹ãƒ¬ãƒƒãƒ‰ãŒçµ‚äº†ã™ã‚‹ã®ã‚’å¾…æ©Ÿ
    if st.session_state.response_thread and st.session_state.response_thread.is_alive():
        logger.info("æ–‡å­—èµ·ã“ã—å‡¦ç†ã‚¹ãƒ¬ãƒƒãƒ‰ã®çµ‚äº†ã‚’å¾…æ©Ÿã—ã¾ã™ã€‚")
        st.session_state.response_thread.join(timeout=1.0)
    
    logger.info("éŸ³å£°èªè­˜ã‚’åœæ­¢ã—ã¾ã—ãŸã€‚")

def clear_history():
    """
    ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã™ã‚‹
    """
    global _transcript_queue, _transcripts, _responses, _current_transcript, _update_ui, _accumulated_context
    
    logger.info("ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã™ã€‚")
    
    _transcripts = []
    _responses = []
    _current_transcript = ""
    _accumulated_context = ""  # è“„ç©ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚‚ã‚¯ãƒªã‚¢
    
    # ã‚­ãƒ¥ãƒ¼ã‚’ã‚¯ãƒªã‚¢
    while not _transcript_queue.empty():
        _transcript_queue.get()
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚‚æ›´æ–°
    st.session_state.transcripts = []
    st.session_state.responses = []
    st.session_state.current_transcript = ""
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã«çŠ¶æ…‹ã‚’ä¿å­˜
    save_state_to_file()
    
    _update_ui = True
    
    logger.info("ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚")

def update_session_state():
    """
    ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‹ã‚‰ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’æ›´æ–°ã™ã‚‹
    """
    global _transcripts, _responses, _current_transcript, _current_response, _force_update, _is_generating
    
    logger.info(f"ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’æ›´æ–°ã—ã¾ã™ã€‚è»¢è¨˜æ•°: {len(_transcripts)}, å¿œç­”æ•°: {len(_responses)}")
    
    # è»¢è¨˜ã¨å¿œç­”ã®æ•°ãŒä¸€è‡´ã—ãªã„å ´åˆã¯èª¿æ•´
    if len(_transcripts) > len(_responses):
        logger.warning(f"è»¢è¨˜æ•°({len(_transcripts)})ãŒå¿œç­”æ•°({len(_responses)})ã‚ˆã‚Šå¤šã„ã§ã™ã€‚èª¿æ•´ã—ã¾ã™ã€‚")
        _transcripts = _transcripts[:len(_responses)]
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æœ€æ–°ã®çŠ¶æ…‹ã‚’èª­ã¿è¾¼ã‚€
    load_state_from_file()
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’æ›´æ–°
    st.session_state.transcripts = _transcripts.copy()
    st.session_state.responses = _responses.copy()
    st.session_state.current_transcript = _current_transcript
    st.session_state.current_response = _current_response
    st.session_state.is_generating = _is_generating
    st.session_state.last_update_time = time.time()
    _force_update = False
    
    logger.info(f"ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚st.session_state.transcripts: {len(st.session_state.transcripts)}, st.session_state.responses: {len(st.session_state.responses)}")

def save_conversation_to_html():
    """
    ä¼šè©±å±¥æ­´ã‚’HTMLå½¢å¼ã§ä¿å­˜ã™ã‚‹
    """
    global _transcripts, _responses
    
    if not _transcripts or not _responses:
        logger.info("ä¿å­˜ã™ã‚‹ä¼šè©±å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return None
    
    try:
        # HTMLãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        html_template = """
        <!DOCTYPE html>
        <html>
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>ä¼šè©±å±¥æ­´</title>
            <style>
                body {
                    font-family: Arial, sans-serif;
                    max-width: 800px;
                    margin: 0 auto;
                    padding: 20px;
                    background-color: #f5f5f5;
                }
                .chat-container {
                    display: flex;
                    flex-direction: column;
                    gap: 15px;
                }
                .message {
                    display: flex;
                    margin-bottom: 10px;
                }
                .user-message {
                    justify-content: flex-end;
                }
                .ai-message {
                    justify-content: flex-start;
                }
                .message-bubble {
                    max-width: 70%;
                    padding: 10px 15px;
                    border-radius: 18px;
                    box-shadow: 0 1px 2px rgba(0,0,0,0.1);
                }
                .user-bubble {
                    background-color: #dcf8c6;
                    border-bottom-right-radius: 5px;
                }
                .ai-bubble {
                    background-color: #ffffff;
                    border-bottom-left-radius: 5px;
                }
                .timestamp {
                    font-size: 0.7em;
                    color: #999;
                    margin-top: 5px;
                    text-align: right;
                }
                h1 {
                    text-align: center;
                    color: #333;
                }
            </style>
        </head>
        <body>
            <h1>ä¼šè©±å±¥æ­´</h1>
            <div class="chat-container">
                {chat_content}
            </div>
        </body>
        </html>
        """
        
        # ä¼šè©±å†…å®¹ã‚’ç”Ÿæˆ
        chat_content = ""
        timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
        
        for i in range(len(_transcripts)):
            # HTMLã‚¨ã‚¹ã‚±ãƒ¼ãƒ—å‡¦ç†
            user_text = _transcripts[i].replace("<", "&lt;").replace(">", "&gt;")
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            chat_content += f"""
            <div class="message user-message">
                <div class="message-bubble user-bubble">
                    <div>{user_text}</div>
                    <div class="timestamp">{timestamp}</div>
                </div>
            </div>
            """
            
            # AIãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            if i < len(_responses):
                # HTMLã‚¨ã‚¹ã‚±ãƒ¼ãƒ—å‡¦ç†
                ai_text = _responses[i].replace("<", "&lt;").replace(">", "&gt;")
                
                chat_content += f"""
                <div class="message ai-message">
                    <div class="message-bubble ai-bubble">
                        <div>{ai_text}</div>
                        <div class="timestamp">{timestamp}</div>
                    </div>
                </div>
                """
        
        # HTMLã‚’ç”Ÿæˆ
        html_content = html_template.format(chat_content=chat_content)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        filename = f"conversation_{time.strftime('%Y%m%d_%H%M%S')}.html"
        
        # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        with open(filename, "w", encoding="utf-8") as f:
            f.write(html_content)
        
        logger.info(f"ä¼šè©±å±¥æ­´ã‚’HTMLãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸ: {filename}")
        return filename
    except Exception as e:
        error_msg = f"ä¼šè©±å±¥æ­´ã®HTMLä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
        logger.error(error_msg)
        return None

def main():
    """
    ãƒ¡ã‚¤ãƒ³é–¢æ•°
    """
    global _transcripts, _responses, _current_transcript, _current_response, _update_ui, _last_ui_update_time, _force_update, _is_generating
    
    logger.info("ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
    
    st.set_page_config(
        page_title="ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°ä¼šè©±",
        page_icon="ğŸ¤",
        layout="wide"
    )
    
    st.title("ğŸ¤ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°ä¼šè©±")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰çŠ¶æ…‹ã‚’èª­ã¿è¾¼ã‚€
    load_state_from_file()
    
    # èµ·å‹•æ™‚ã«ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‹ã‚‰ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’æ›´æ–°
    update_session_state()
    logger.info(f"èµ·å‹•æ™‚ã«ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚è»¢è¨˜æ•°: {len(st.session_state.transcripts)}, å¿œç­”æ•°: {len(st.session_state.responses)}")
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    with st.sidebar:
        st.header("è¨­å®š")
        
        # éŸ³å£°èªè­˜ã®é–‹å§‹/åœæ­¢ãƒœã‚¿ãƒ³
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ğŸ¤ éŒ²éŸ³é–‹å§‹", use_container_width=True, disabled=st.session_state.is_listening):
                logger.info("éŒ²éŸ³é–‹å§‹ãƒœã‚¿ãƒ³ãŒã‚¯ãƒªãƒƒã‚¯ã•ã‚Œã¾ã—ãŸã€‚")
                start_listening()
                st.rerun()  # UIã‚’å³æ™‚æ›´æ–°
        
        with col2:
            if st.button("â¹ï¸ éŒ²éŸ³åœæ­¢", use_container_width=True, disabled=not st.session_state.is_listening):
                logger.info("éŒ²éŸ³åœæ­¢ãƒœã‚¿ãƒ³ãŒã‚¯ãƒªãƒƒã‚¯ã•ã‚Œã¾ã—ãŸã€‚")
                stop_listening()
                st.rerun()  # UIã‚’å³æ™‚æ›´æ–°
        
        # å±¥æ­´ã‚¯ãƒªã‚¢ãƒœã‚¿ãƒ³
        if st.button("ğŸ—‘ï¸ å±¥æ­´ã‚¯ãƒªã‚¢", use_container_width=True):
            logger.info("å±¥æ­´ã‚¯ãƒªã‚¢ãƒœã‚¿ãƒ³ãŒã‚¯ãƒªãƒƒã‚¯ã•ã‚Œã¾ã—ãŸã€‚")
            clear_history()
            st.rerun()  # UIã‚’å³æ™‚æ›´æ–°
        
        # ä¼šè©±å±¥æ­´ä¿å­˜ãƒœã‚¿ãƒ³
        if st.button("ğŸ’¾ ä¼šè©±å±¥æ­´ã‚’ä¿å­˜", use_container_width=True):
            logger.info("ä¼šè©±å±¥æ­´ä¿å­˜ãƒœã‚¿ãƒ³ãŒã‚¯ãƒªãƒƒã‚¯ã•ã‚Œã¾ã—ãŸã€‚")
            filename = save_conversation_to_html()
            if filename:
                st.success(f"ä¼šè©±å±¥æ­´ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filename}")
            else:
                st.error("ä¼šè©±å±¥æ­´ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        
        # æ‰‹å‹•æ›´æ–°ãƒœã‚¿ãƒ³
        if st.button("ğŸ”„ ç”»é¢æ›´æ–°", use_container_width=True):
            logger.info("ç”»é¢æ›´æ–°ãƒœã‚¿ãƒ³ãŒã‚¯ãƒªãƒƒã‚¯ã•ã‚Œã¾ã—ãŸã€‚")
            update_session_state()
            st.rerun()  # UIã‚’å³æ™‚æ›´æ–°
        
        # è‡ªå‹•æ›´æ–°ã®è¨­å®š
        auto_refresh = st.checkbox("è‡ªå‹•æ›´æ–°ï¼ˆ1ç§’ã”ã¨ï¼‰", value=True)
        
        # æ›´æ–°ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿
        if _update_ui:
            st.success("æ–°ã—ã„ä¼šè©±ãŒè¿½åŠ ã•ã‚Œã¾ã—ãŸï¼")
        
        st.divider()
        
        # çŠ¶æ…‹è¡¨ç¤º
        st.subheader("çŠ¶æ…‹")
        st.write(f"éŒ²éŸ³ä¸­: {'ã¯ã„' if st.session_state.is_listening else 'ã„ã„ãˆ'}")
        st.write(f"å¿œç­”ç”Ÿæˆä¸­: {'ã¯ã„' if st.session_state.is_generating else 'ã„ã„ãˆ'}")
        st.write(f"æœ€çµ‚æ›´æ–°: {time.strftime('%H:%M:%S', time.localtime(st.session_state.last_update_time))}")
        
        # ãƒ­ã‚°è¡¨ç¤º
        st.subheader("ãƒ­ã‚°")
        if os.path.exists("app.log"):
            with open("app.log", "r") as f:
                log_content = f.read()
                st.text_area("æœ€æ–°ã®ãƒ­ã‚°", log_content[-5000:], height=200)
        
        st.divider()
        
        # ä½¿ã„æ–¹
        st.subheader("ä½¿ã„æ–¹")
        st.markdown("""
        1. ã€ŒéŒ²éŸ³é–‹å§‹ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦éŸ³å£°èªè­˜ã‚’é–‹å§‹ã—ã¾ã™ã€‚
        2. ãƒã‚¤ã‚¯ã«å‘ã‹ã£ã¦è©±ã—ã‹ã‘ã¦ãã ã•ã„ã€‚
        3. AIãŒè‡ªå‹•çš„ã«ä¼šè©±ã‚’ç¶™ç¶šã™ã‚‹ã‹ç›¸æ§Œã‚’æ‰“ã¤ã‹åˆ¤æ–­ã—ã¾ã™ã€‚
        4. ã€ŒéŒ²éŸ³åœæ­¢ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦éŸ³å£°èªè­˜ã‚’åœæ­¢ã—ã¾ã™ã€‚
        5. ã€Œå±¥æ­´ã‚¯ãƒªã‚¢ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã™ã€‚
        6. ã€Œä¼šè©±å±¥æ­´ã‚’ä¿å­˜ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦HTMLå½¢å¼ã§ä¿å­˜ã—ã¾ã™ã€‚
        """)
    
    # è‡ªå‹•æ›´æ–°ã®ãŸã‚ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
    update_placeholder = st.empty()
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°æ™‚åˆ»ã‚’ç¢ºèª
    try:
        file_modified_time = os.path.getmtime(_STATE_FILE) if os.path.exists(_STATE_FILE) else 0
    except:
        file_modified_time = 0
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ›´æ–°ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’æ›´æ–°
    if file_modified_time > st.session_state.last_update_time:
        logger.info("ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ›´æ–°ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’æ›´æ–°ã—ã¾ã™ã€‚")
        load_state_from_file()
        update_session_state()
    
    # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‹ã‚‰ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’æ›´æ–°ï¼ˆUIã®æ›´æ–°ãƒ•ãƒ©ã‚°ãŒã‚»ãƒƒãƒˆã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
    current_time = time.time()
    if _update_ui or _force_update or (auto_refresh and current_time - st.session_state.last_update_time > 1):
        update_session_state()
        _update_ui = False
        
        # å¼·åˆ¶æ›´æ–°ãƒ•ãƒ©ã‚°ãŒã‚»ãƒƒãƒˆã•ã‚Œã¦ã„ã‚‹å ´åˆã¾ãŸã¯è‡ªå‹•æ›´æ–°ãŒæœ‰åŠ¹ã§æœ€å¾Œã®UIæ›´æ–°ã‹ã‚‰3ç§’ä»¥ä¸ŠçµŒéã—ã¦ã„ã‚‹å ´åˆã¯å†èª­ã¿è¾¼ã¿
        if _force_update or (auto_refresh and current_time - _last_ui_update_time < 10 and _last_ui_update_time > st.session_state.last_update_time):
            logger.info("UIã‚’å†èª­ã¿è¾¼ã¿ã—ã¾ã™ã€‚")
            time.sleep(0.1)
            st.rerun()
    
    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
    st.sidebar.subheader("ãƒ‡ãƒãƒƒã‚°æƒ…å ±")
    st.sidebar.write(f"ã‚°ãƒ­ãƒ¼ãƒãƒ«è»¢è¨˜æ•°: {len(_transcripts)}")
    st.sidebar.write(f"ã‚°ãƒ­ãƒ¼ãƒãƒ«å¿œç­”æ•°: {len(_responses)}")
    st.sidebar.write(f"ã‚»ãƒƒã‚·ãƒ§ãƒ³è»¢è¨˜æ•°: {len(st.session_state.transcripts)}")
    st.sidebar.write(f"ã‚»ãƒƒã‚·ãƒ§ãƒ³å¿œç­”æ•°: {len(st.session_state.responses)}")
    if os.path.exists(_STATE_FILE):
        st.sidebar.write(f"çŠ¶æ…‹ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°: {time.strftime('%H:%M:%S', time.localtime(file_modified_time))}")
    
    # ãƒ¡ã‚¤ãƒ³ç”»é¢ - å¹ãå‡ºã—å½¢å¼ã®ä¼šè©±è¡¨ç¤º
    st.header("ä¼šè©±å±¥æ­´")
    
    # CSSã‚¹ã‚¿ã‚¤ãƒ«ã‚’è¿½åŠ 
    st.markdown("""
    <style>
    .chat-container {
        display: flex;
        flex-direction: column;
        gap: 15px;
        margin-bottom: 20px;
    }
    .message {
        display: flex;
        margin-bottom: 10px;
    }
    .user-message {
        justify-content: flex-end;
    }
    .ai-message {
        justify-content: flex-start;
    }
    .message-bubble {
        max-width: 70%;
        padding: 10px 15px;
        border-radius: 18px;
        box-shadow: 0 1px 2px rgba(0,0,0,0.1);
    }
    .user-bubble {
        background-color: #dcf8c6;
        border-bottom-right-radius: 5px;
    }
    .ai-bubble {
        background-color: #ffffff;
        border: 1px solid #e5e5e5;
        border-bottom-left-radius: 5px;
    }
    .timestamp {
        font-size: 0.7em;
        color: #999;
        margin-top: 5px;
        text-align: right;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # ç¾åœ¨ã®æ–‡å­—èµ·ã“ã—ã¨å¿œç­”ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤ºï¼‰
    chat_container = st.container()
    
    with chat_container:
        # ç¾åœ¨ã®æ–‡å­—èµ·ã“ã—ãŒã‚ã‚‹å ´åˆã¯è¡¨ç¤º
        if st.session_state.current_transcript:
            # HTMLã‚¨ã‚¹ã‚±ãƒ¼ãƒ—å‡¦ç†
            current_transcript_escaped = st.session_state.current_transcript.replace("<", "&lt;").replace(">", "&gt;")
            
            st.markdown(f"""
            <div class="chat-container">
                <div class="message user-message">
                    <div class="message-bubble user-bubble">
                        <div>{current_transcript_escaped}</div>
                        <div class="timestamp">ç¾åœ¨ã®ç™ºè¨€</div>
                    </div>
                </div>
            </div>
            """, unsafe_allow_html=True)
        
        # å¿œç­”ç”Ÿæˆä¸­ã®å ´åˆã¯è¡¨ç¤º
        if st.session_state.is_generating and st.session_state.current_response:
            # HTMLã‚¨ã‚¹ã‚±ãƒ¼ãƒ—å‡¦ç†
            current_response_escaped = st.session_state.current_response.replace("<", "&lt;").replace(">", "&gt;")
            
            st.markdown(f"""
            <div class="chat-container">
                <div class="message ai-message">
                    <div class="message-bubble ai-bubble">
                        <div>{current_response_escaped}</div>
                        <div class="timestamp">ç”Ÿæˆä¸­...</div>
                    </div>
                </div>
            </div>
            """, unsafe_allow_html=True)
        
        # éå»ã®ä¼šè©±å±¥æ­´ã‚’è¡¨ç¤º
        if st.session_state.transcripts and st.session_state.responses:
            chat_history = ""
            for i in range(len(st.session_state.transcripts) - 1, -1, -1):
                # HTMLã‚¨ã‚¹ã‚±ãƒ¼ãƒ—å‡¦ç†
                transcript_escaped = st.session_state.transcripts[i].replace("<", "&lt;").replace(">", "&gt;")
                
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
                chat_history += f"""
                <div class="message user-message">
                    <div class="message-bubble user-bubble">
                        <div>{transcript_escaped}</div>
                    </div>
                </div>
                """
                
                # AIãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
                if i < len(st.session_state.responses):
                    # HTMLã‚¨ã‚¹ã‚±ãƒ¼ãƒ—å‡¦ç†
                    response_escaped = st.session_state.responses[i].replace("<", "&lt;").replace(">", "&gt;")
                    
                    chat_history += f"""
                    <div class="message ai-message">
                        <div class="message-bubble ai-bubble">
                            <div>{response_escaped}</div>
                        </div>
                    </div>
                    """
            
            st.markdown(f'<div class="chat-container">{chat_history}</div>', unsafe_allow_html=True)
        else:
            st.info("ä¼šè©±å±¥æ­´ã¯ã¾ã ã‚ã‚Šã¾ã›ã‚“ã€‚ãƒã‚¤ã‚¯ã«å‘ã‹ã£ã¦è©±ã—ã‹ã‘ã¦ãã ã•ã„ã€‚")
    
    # è‡ªå‹•æ›´æ–°ã®å‡¦ç†
    if auto_refresh:
        with update_placeholder:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°æ™‚åˆ»ã‚’ç¢ºèª
            try:
                current_file_modified_time = os.path.getmtime(_STATE_FILE) if os.path.exists(_STATE_FILE) else 0
            except:
                current_file_modified_time = 0
            
            # å¿œç­”ç”Ÿæˆä¸­ã¾ãŸã¯æœ€å¾Œã®æ›´æ–°ã‹ã‚‰1ç§’ä»¥ä¸ŠçµŒéã—ã¦ã„ã‚‹å ´åˆã¯å†èª­ã¿è¾¼ã¿
            if st.session_state.is_generating or current_file_modified_time > file_modified_time or time.time() - st.session_state.last_update_time > 1:
                st.session_state.last_update_time = time.time()
                time.sleep(0.1)
                st.rerun()

if __name__ == "__main__":
    logger.info("ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’èµ·å‹•ã—ã¾ã™ã€‚")
    main() 